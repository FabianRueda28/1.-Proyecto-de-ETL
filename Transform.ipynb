{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import openpyxl as opx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la transformación de los datos semiestructurados vamos a utilizar pandas y transformarlos en Dataframes con el fín de limpiarlos y procesarlos mas rapido. \n",
    "La otra librería a utlizar es openpyxl para lectura de archivos xlsx. \n",
    "\n",
    "De ahora en adelante vamos a llamar a los archivos datasets, así será mas facil identificar los archivos. \n",
    "\n",
    "La ruta de acceso depende de la carpeta en la cual descargaron los archivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos un archivo Json a CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = pd.read_json(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_2.json')\n",
    "dataset_2.to_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_2.csv', index = False)\n",
    "dataset_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos un archivo Txt a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3 = pd.read_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_3.txt')\n",
    "dataset_3.to_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_3_txt.csv', index = False)\n",
    "dataset_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos un archivo Xlsx a CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_4 = pd.read_excel(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_4.xlsx')\n",
    "dataset_4.to_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_4.csv', index = False)\n",
    "dataset_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5 = pd.read_excel(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_5.xlsx')\n",
    "dataset_5.to_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_5.csv', index = False)\n",
    "dataset_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos un archivo Parquet a CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertir un archivo parquet a csv\n",
    "producto = pd.read_parquet(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\producto.parquet')\n",
    "producto.to_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\producto.csv', index = False)\n",
    "producto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de los Datos \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que vamos hacer ahora es cargar todos los CSV para empezar a trabajarlos. \n",
    "La ruta de acceso depende de la carpeta en la cual descargaron los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = pd.read_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_1.csv')\n",
    "dataset_2 = pd.read_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_2.csv')\n",
    "dataset_3 = pd.read_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_3_txt.csv')\n",
    "dataset_4 = pd.read_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_4.csv')\n",
    "dataset_5 = pd.read_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_5.csv')\n",
    "producto = pd.read_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\producto.csv')\n",
    "sucursal = pd.read_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\sucursal.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el dataset_1 lo primero que hicimos fue contar las filas vacias y eliminarlas para evitar una confusión cuado carguemos los datos en el DataBase. \n",
    "Despues guardamos los datos con las transformaciones ya hechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1.isnull().sum()\n",
    "dataset_1.dropna(inplace=True)\n",
    "dataset_1.isnull().sum() \n",
    "\n",
    "#guardar cambios\n",
    "dataset_1.to_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_1.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el dataset_2 lo que hicimos fue buscar valores nulos en las columnas y volverlos tipo int con valor 0 para no perjudicar los datos y así poder cargarlos en el DataBase y evitar errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2.isnull().sum()\n",
    "dataset_2.fillna(0, inplace=True)\n",
    "dataset_2.isnull().sum() \n",
    "\n",
    "#guardar cambios\n",
    "dataset_2.to_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el dataset_3 continuamos buscando valores nulos en las columnas y volverlos tipo int con valor 0 para no perjudicar los datos y así poder cargarlos en el DataBase y evitar errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3.isnull().sum()\n",
    "dataset_3.fillna(0, inplace=True)\n",
    "dataset_3.isnull().sum() \n",
    "\n",
    "#guardar cambios\n",
    "dataset_3.to_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_3_txt.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminanos los - en las columnas producto_id y sucursal_id para realizar la carga en el darabase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminar - de producto y sucursal\n",
    "dataset_3['producto_id'] = dataset_3['producto_id'].str.replace('-','')\n",
    "dataset_3['sucursal_id'] = dataset_3['sucursal_id'].str.replace('-','')\n",
    "\n",
    "#guardar cambios\n",
    "dataset_3.to_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_3_txt.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el dataset_4 nuevamente buscamos valores nulos en las columnas y volverlos tipo int con valor 0 para no perjudicar los datos y así poder cargarlos en el DataBase y evitar errores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_4.isnull().sum()\n",
    "dataset_4.fillna(0, inplace=True)\n",
    "dataset_4.isnull().sum()\n",
    "\n",
    "#guardar cambios \n",
    "dataset_4.to_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_4.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡¡ En el Dataset_4 y en el Dataset_5 las columnas están inversas a los otros datasets. !!\n",
    "\n",
    "Continuando en el dataset_4 despues de reemplazar los valores nulos pasamos a reorganizar las columnas del dataset, esto con el fin que cuando carguemos los datos en el DataBase tengan el mismo ordén que los otros datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_4 = dataset_4.reindex(columns=['precio', 'producto_id', 'sucursal_id'])\n",
    "dataset_4.head(10) \n",
    "\n",
    "#guardar cambios\n",
    "dataset_4.to_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_4.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el dataset_5 lo primero que hacemos es buscar nuevamente valores nulos y reemplazarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5.isnull().sum()\n",
    "dataset_5.fillna(0, inplace=True)\n",
    "dataset_5.isnull().sum()\n",
    "\n",
    "#guardar cambios en el csv\n",
    "dataset_5.to_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_5.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues pasamos a limpiar los datos, como reemplazar valores como 00:00:00 de la columna sucursal_id por espacios en blanco, con esto vamos normalizando los datos de la columna para poder hacer luego la ingesta en el DataBase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5['sucursal_id'] = dataset_5['sucursal_id'].str.replace('00:00:00', '')\n",
    "dataset_5.head(10)\n",
    "\n",
    "#guardar cambios en el csv\n",
    "dataset_5.to_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_5.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Igual forma cambiamos la posición de las columnas para unificar luego los datos en la Base de Datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5 = dataset_5.reindex(columns=['precio', 'producto_id', 'sucursal_id'])\n",
    "dataset_5.head(10)  \n",
    "\n",
    "#guardar cambios en el csv\n",
    "dataset_5.to_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_5.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una funcion para eliminar todos los datos horarios, y ordenar bien los datos de cadena colocando el los codigos en el orden correcto, que se desordenaron al cambiarlo a fecha para luego continuar con el proceso de transformacion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertimos a str toda la col\n",
    "dataset_5['sucursal_id']=dataset_5['sucursal_id'].astype(str)\n",
    "\n",
    "#Ahora implementamos la funcion\n",
    "def norm_time (x):\n",
    "    if ' ' in x:\n",
    "        x = x.split(' ')[0]\n",
    "        a = x.split('-')[0]\n",
    "        b = x.split('-')[1]\n",
    "        c = x.split('-')[2]\n",
    "        x = c + '-' + b + '-' + a\n",
    "    return x\n",
    "\n",
    "dataset_5['sucursal_id']=dataset_5['sucursal_id'].apply(norm_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5['sucursal_id'] = dataset_5['sucursal_id'].str.replace('5218-03-09', '09-03-5218')\n",
    "dataset_5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo guardamos los cambios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardar cambios\n",
    "dataset_5.to_csv(r'C:\\Users\\USER\\Desktop\\Proyects to Henry\\PI01_DATA_ENGINEERING\\Datasets\\precios_semana_5.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df95319d8ce4e1d89f5365ae10992bc1f65da593082b1d264e8f529830ec2f02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
